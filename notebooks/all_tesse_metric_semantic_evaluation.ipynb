{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "\n",
    "from evaluation.metric_semantic_evaluation import MeshEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL PATHS BELOW\n",
    "artifacts_path = \"/home/tonirv/Documents/uHumans2_VIO_vxblx/\"\n",
    "gt_meshes_path = \"/home/tonirv/datasets/uHumans2/uHumans dataset V2.0 GT Meshes/\"\n",
    "semantic_labels_csvs_path = \"/home/tonirv/Code/ROS/kimera_ws/src/Kimera-Semantics/kimera_semantics_ros/cfg/\"\n",
    "visualize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mesh_evaluation(scene_type, human_size, number_of_mesh_samples):\n",
    "    est_mesh_base_path = artifacts_path + \"/{}_scene/uHumans2_{}_s1_{}h/\".format(scene_type, scene_type, human_size)\n",
    "    est_mesh_names = ['mesh_DVIO.ply', \n",
    "                      'mesh_gt.ply', \n",
    "                      'mesh_DVIO_wo_DM.ply', \n",
    "                      'mesh_gt_wo_DM.ply']\n",
    "    \n",
    "    for est_mesh_name in est_mesh_names:\n",
    "        est_mesh_path = est_mesh_base_path + est_mesh_name\n",
    "        print(\"EVAL: {} in {} scene\".format(est_mesh_name, scene_type))\n",
    "        if not os.path.exists(est_mesh_path):\n",
    "            print(\"Path to {} doesn't exist: {}\".format(est_mesh_name, est_mesh_path))\n",
    "            continue\n",
    "        mesh_eval = MeshEvaluator(est_mesh_path, gt_mesh_path, semantic_labels_csv_path, visualize)\n",
    "        inlier_rmse, semantic_accuracy = mesh_eval.compare_meshes(number_of_mesh_samples)\n",
    "        print(\"Inlier RMSE [m]: \", inlier_rmse)\n",
    "        print(\"Semantic Accuracy [%]: \", semantic_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartment Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mesh_path = gt_meshes_path + \"apartment.ply\"\n",
    "semantic_labels_csv_path = semantic_labels_csvs_path + \"tesse_multiscene_archviz1_segmentation_mapping.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartment S1 00h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: mesh_DVIO.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.028118, inlier_rmse = 1.042821, and correspondence_set size of 13942\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.06844719229\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cb2e5bfebd47afb3db6ad33e56546e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20423.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "91.3235078098\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.0684471922917758)\n",
      "('Semantic Accuracy [%]: ', 91.32350780982226)\n",
      "EVAL: mesh_gt.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.024532, inlier_rmse = 1.055854, and correspondence_set size of 11950\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.07389866518\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061ffcb7f00243a8b466af3bd9e2f357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20067.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "82.3092639657\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.073898665179196)\n",
      "('Semantic Accuracy [%]: ', 82.30926396571485)\n",
      "EVAL: mesh_DVIO_wo_DM.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.030622, inlier_rmse = 1.070821, and correspondence_set size of 15183\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.05557548192\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c720c5f9024eb691ba5c1bc7bec80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25295.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "89.4326942083\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.0555754819178538)\n",
      "('Semantic Accuracy [%]: ', 89.43269420834157)\n",
      "EVAL: mesh_gt_wo_DM.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.024928, inlier_rmse = 1.053369, and correspondence_set size of 12143\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.08319244069\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1fd41e43bd4d8f9295280dc31003c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28520.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "85.2033660589\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.0831924406932927)\n",
      "('Semantic Accuracy [%]: ', 85.20336605890603)\n"
     ]
    }
   ],
   "source": [
    "run_mesh_evaluation(\"apartment\", \"00\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartment S1 01h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: mesh_DVIO.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.027024, inlier_rmse = 1.050978, and correspondence_set size of 13347\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.05395554998\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9ae14466c7457aa9878b4723c67e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19430.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "87.0818322182\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.0539555499848483)\n",
      "('Semantic Accuracy [%]: ', 87.08183221821925)\n",
      "EVAL: mesh_gt.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.032863, inlier_rmse = 1.097591, and correspondence_set size of 15991\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.07031409966\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f3f53099064361932528cec2af74e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28448.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "85.6861642295\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.070314099660517)\n",
      "('Semantic Accuracy [%]: ', 85.68616422947132)\n",
      "EVAL: mesh_DVIO_wo_DM.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.025967, inlier_rmse = 1.074321, and correspondence_set size of 12841\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.09884341227\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef772f96de3b48bea7a939be120353e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28536.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "87.3072610036\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.0988434122705997)\n",
      "('Semantic Accuracy [%]: ', 87.30726100364453)\n",
      "EVAL: mesh_gt_wo_DM.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.026223, inlier_rmse = 1.069006, and correspondence_set size of 12774\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.07580405193\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccb450da425431cabf427347e2bb755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28698.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "85.1906056171\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.0758040519300793)\n",
      "('Semantic Accuracy [%]: ', 85.19060561711618)\n"
     ]
    }
   ],
   "source": [
    "run_mesh_evaluation(\"apartment\", \"01\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartment S1 02h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: mesh_DVIO.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.028083, inlier_rmse = 1.050084, and correspondence_set size of 13970\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.06025809329\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6f49b851514d5b923d469af82f07d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20702.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "84.2962032654\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.0602580932895922)\n",
      "('Semantic Accuracy [%]: ', 84.29620326538499)\n",
      "EVAL: mesh_gt.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.024576, inlier_rmse = 1.059144, and correspondence_set size of 11951\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.0460915398\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60701b771fa3482f94490b6fa04c0a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20195.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "83.3622183709\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.046091539803593)\n",
      "('Semantic Accuracy [%]: ', 83.36221837088388)\n",
      "EVAL: mesh_DVIO_wo_DM.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.026015, inlier_rmse = 1.050890, and correspondence_set size of 12951\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.06277398\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f30a6c9300a4529b9a138c3bc7e428b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20583.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "91.5075547782\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.0627739800031002)\n",
      "('Semantic Accuracy [%]: ', 91.50755477821502)\n",
      "EVAL: mesh_gt_wo_DM.ply in apartment scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.022845, inlier_rmse = 1.077375, and correspondence_set size of 11115\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.05833632437\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f25e0a6836e4b3d8dccdcb34fd44d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19445.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "79.6760092569\n",
      " \n",
      "('Inlier RMSE [m]: ', 1.0583363243679953)\n",
      "('Semantic Accuracy [%]: ', 79.67600925687837)\n"
     ]
    }
   ],
   "source": [
    "run_mesh_evaluation(\"apartment\", \"02\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Office Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mesh_path = gt_meshes_path + \"office.ply\"\n",
    "semantic_labels_csv_path = semantic_labels_csvs_path + \"tesse_multiscene_office2_segmentation_mapping.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Office Scene 00h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: mesh_DVIO.ply in office scene\n",
      "Init MeshEvaluator\n",
      "Loading Ground-truth mesh...\n",
      "Loading Estimated mesh...\n",
      "Initial registration\n",
      "registration::RegistrationResult with fitness = 0.169269, inlier_rmse = 1.094791, and correspondence_set size of 364856\n",
      "Access transformation to get result.\n",
      "Apply point-to-point ICP\n",
      "Done with point-to-point ICP\n",
      "Geometric inlier RMSE [m]: \n",
      "1.06745891324\n",
      " \n",
      "Semantic Accuracy [%]: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84a434b93fc46af8d5afc7b9b849515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=388459.0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-db1f99c8117d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_mesh_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"office\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"00\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-aa18a70ce84b>\u001b[0m in \u001b[0;36mrun_mesh_evaluation\u001b[0;34m(scene_type, human_size, number_of_mesh_samples)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmesh_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeshEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_mesh_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mesh_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic_labels_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0minlier_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_meshes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_mesh_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inlier RMSE [m]: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minlier_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Semantic Accuracy [%]: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonirv/Code/Kimera-VIO-Evaluation/venv/local/lib/python2.7/site-packages/evaluation/metric_semantic_evaluation.pyc\u001b[0m in \u001b[0;36mcompare_meshes\u001b[0;34m(self, number_of_mesh_samples)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Calculate semantic metrics using the ICP correspondences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Semantic Accuracy [%]: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0msemantic_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_corresp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_pcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_pcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_p2p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrespondence_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonirv/Code/Kimera-VIO-Evaluation/venv/local/lib/python2.7/site-packages/evaluation/metric_semantic_evaluation.pyc\u001b[0m in \u001b[0;36mcalc_corresp\u001b[0;34m(self, est_pcl, gt_pcl, correspondences)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrespondence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_pcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrespondence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_pcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mest_label_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_from_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_pcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorrespondence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0mgt_label_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_from_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_pcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorrespondence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mest_label_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgt_label_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonirv/Code/Kimera-VIO-Evaluation/venv/local/lib/python2.7/site-packages/evaluation/metric_semantic_evaluation.pyc\u001b[0m in \u001b[0;36mlabel_from_color\u001b[0;34m(self, color)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# TODO(Toni): you are comparing floats with == ......\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         label_list = self.normalized_df.loc[(self.normalized_df['normalized_red'] == norm_r) &\n\u001b[0;32m--> 150\u001b[0;31m                                             \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_green'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnorm_g\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                                             (self.normalized_df['normalized_blue'] == norm_b)]['id'].unique().tolist()\n\u001b[1;32m    152\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonirv/Code/Kimera-VIO-Evaluation/venv/local/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n",
      "\u001b[0;32m/home/tonirv/Code/Kimera-VIO-Evaluation/venv/local/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1646\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1649\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonirv/Code/Kimera-VIO-Evaluation/venv/local/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_Unspecified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseterrcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonirv/Code/Kimera-VIO-Evaluation/venv/local/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m     \u001b[0mpyvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeterrobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2737\u001b[0;31m     \u001b[0mold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdivide\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonirv/Code/Kimera-VIO-Evaluation/venv/local/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36mgeterr\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m     \"\"\"\n\u001b[0;32m-> 2796\u001b[0;31m     \u001b[0mmaskvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeterrobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2797\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_mesh_evaluation(\"office\", \"00\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Office Scene 06h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mesh_evaluation(\"office\", \"06\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Office Scene 12h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mesh_evaluation(\"office\", \"12\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nieghborhood Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mesh_path = gt_meshes_path + \"neighborhood.ply\"\n",
    "semantic_labels_csv_path = semantic_labels_csvs_path + \"tesse_multiscene_neighborhood1_segmentation_mapping.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nieghborhood Scene 00h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mesh_evaluation(\"neighborhood\", \"00\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nieghborhood Scene 24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mesh_evaluation(\"neighborhood\", \"24\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nieghborhood Scene 36h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mesh_evaluation(\"neighborhood\", \"36\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subway Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mesh_path = gt_meshes_path + \"subway.ply\"\n",
    "semantic_labels_csv_path = semantic_labels_csvs_path + \"tesse_multiscene_underground1_segmentation_mapping.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subway Scene 00h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mesh_evaluation(\"subway\", \"00\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subway Scene 24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mesh_evaluation(\"subway\", \"24\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subway Scene 36h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mesh_evaluation(\"subway\", \"36\", 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
